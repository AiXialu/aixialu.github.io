---
title: deep-learning-1
date: 2020-07-26 16:37:05
tags:
---

## [Deep Learning](http://lindongding.com/2018/05/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0/)

<!-- More -->

CNN 其实说利用局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示

![](https://pic2.zhimg.com/80/v2-8555de443211e31f6e3967fe0fab83b3_720w.jpg)

### CNN 基本手段

>CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。

- 局部连接使网络可以提取数据的局部特征；
- 权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；
- 池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：

![](https://picb.zhimg.com/80/v2-27961b1ce1d39d970fae7e40fd99edf3_720w.jpg)

- local-conv

    CNN 并不是所有的conv都是全图的conv的，可以进行local-conv的，比如说人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

>什麽样的资料集不适合用深度学习?
- 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
- 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。
    
    对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

> 解决过度拟合

- Dropout
- 共线性问题：
    
    共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。

    共线性会造成冗余，导致过拟合。
    
    解决方法：排除变量的相关性／加入权重正则。

> 梯度下降的问题：GD, SGD, Mini-Batch

    梯度下降的优化框架有三种：批量梯度下降（全）GD，随机梯度下降（一）SGD，小批量梯度下降（Mini-Batch)。它们不同之处在于每次学习（更新模型参数）使用的样本个数，每次更新使用不同的样本会导致每次学习的准确性和学习时间不同。

    批量梯度下降其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)，不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。Dauphin指出更严重的问题不是局部极值点，而是鞍点。（目标函数在此点上的梯度（一阶导数）值为 0，但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。）

>优化方法：

- Momentum：要是当前时刻的梯度与历史时刻梯度方向相似，这种趋势在当前时刻则会加强；要是不同，则当前时刻的梯度方向减弱）

- Adagrad：也是一种基于梯度的优化算法，它能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，因此该优化算法适合处理稀疏特征数据。在GloVe中便使用Adagrad来训练得到词向量(Word Embeddings), 频繁出现的单词赋予较小的更新，不经常出现的单词则赋予较大的更新。

- Adam：也是一种不同参数自适应不同学习速率方法，与Adadelta与RMSprop区别在于，它计算历史梯度衰减方式不同，不使用历史平方衰减，其衰减方式类似动量。

> Supervised Learning/Non-Supervised Learning/Transfer-Learning

- 监督学习，数据有label

- 半监督学习：其训练数据的一部分是有标签的，另一部分没有标签，而没标签数据的数量常常极大于有标签数据数量（这也是符合现实情况的）。隐藏在半监督学习下的基本规律在于：数据的分布必然不是完全随机的，通过一些有标签数据的局部特征，以及更多没标签数据的整体分布，就可以得到可以接受甚至是非常好的分类结果。（可以说是弱监督的子集）
- 无监督学习：AutoEncoding，PCA，RF，K-means，GAN等。
- 强化学习：强化学习是一个多次决策的过程，可以形成一个决策链；监督学习只是一个一次决策的过程。
- 迁移学习：迁移学习(Transfer learning) 顾名思义就是就是把已学训练好的模型参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。

> Softmax Loss

Softmax Loss就是Softmax上正确的标签的概率的负对数，这样的话，Loss越小，表示模型效果也越好。而且softmax loss的求导特别方便，只需要将计算的概率向量对应真正结果的那一维减一就好了。具体推导可以看卷积神经网络系列之softmax loss对输入的求导推导，其实基本是简单的求导，学过高数的就可以求。为什么要取指数，第一个原因是要模拟max的行为，所以要让大的更大。第二个原因是需要一个可导的函数。
知识点：现在还是a和b，a>b，如果我们取按照softmax来计算取a和b的概率，那a的softmax值大于b的，所以a会经常取到，而b也会偶尔取到，概率跟它们本来的大小有关。所以说不是max，而是softmax。根据公式:
![](https://img-blog.csdn.net/20170817074525063?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
来举个例子吧。假设一个5分类问题，然后一个样本I的标签y=[0,0,0,1,0]，也就是说样本I的真实标签是4，假设模型预测的结果概率（softmax的输出）p=[0.1,0.15,0.05,0.6,0.1]，可以看出这个预测是对的，那么对应的损失L=-log(0.6)，也就是当这个样本经过这样的网络参数产生这样的预测p时，它的损失是-log(0.6)。那么假设p=[0.15,0.2,0.4,0.1,0.15]，这个预测结果就很离谱了，因为真实标签是4，而你觉得这个样本是4的概率只有0.1（远不如其他概率高，如果是在测试阶段，那么模型就会预测该样本属于类别3），对应损失L=-log(0.1)。那么假设p=[0.05,0.15,0.4,0.3,0.1]，这个预测结果虽然也错了，但是没有前面那个那么离谱，对应的损失L=-log(0.3)。我们知道log函数在输入小于1的时候是个负数，而且log函数是递增函数，所以-log(0.6) < -log(0.3) < -log(0.1)。简单讲就是你预测错比预测对的损失要大，预测错得离谱比预测错得轻微的损失要大。

> Cross Entropy

交叉熵是
![](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Clog+%5Chat%7By%7D_i%5E%7Bk_i%7D+%5Ctag%7B2%7D)

当cross entropy的输入P是softmax的输出时，cross entropy等于softmax loss。Pj是输入的概率向量P的第j个值，所以如果你的概率是通过softmax公式得到的，那么cross entropy就是softmax loss

> MSE(mean squared error)

![](https://www.zhihu.com/equation?tex=+L+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Csum_%7Bk%3D1%7D%5EK++%28y_i%5Ek+-+%5Chat%7By%7D_i%5Ek%29%5E2++%5Ctag%7B1%7D)

分类模型中交叉熵比MSE更合适

- MSE无差别得关注全部类别上预测概率和真实概率的差.

- 交叉熵关注的是正确类别的预测概率.

分类问题中,模型的输出空间是概率分布,但目标输出空间是样例的类别,也就是说我们最终目标是获得正确的类别.

举例来说

1. 如果真实标签是(1, 0, 0),模型1的预测标签是(0.8, 0.2, 0),模型2的是(0.8, 0.1, 0.1),那么MSE-based,就是模型2更好;交叉熵-based认为一样.从最终预测的类别上看,模型1和模型2的真实输出其实是一样的.

2. 再换个角度,MSE对残差大的样例惩罚更大些.比如真实标签分别是(1, 0, 0).模型1的预测标签是(0.8, 0.2, 0),模型2的是(0.9, 0.1, 0).即使输出的标签都是类别0, 但MSE-based算出来模型1的误差是模型2的4倍,而交叉熵-based算出来模型1的误差是模型2的2倍左右.为了弥补模型1在这个样例上的损失,MSE-based需要3个完美预测的样例才能达到和模型2一样的损失,而交叉熵-based只需要一个.实际上,模型输出正确的类别,0.8可能已经是个不错的概率了.

3. 另外MSE的不好的地方：1. 其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失。

## 总结

softmax loss函数是指针对softmax分类器的损失函数，是一种泛指；可以使用mse也可以是cross-entropy loss函数或其他种类；交叉熵损失函数只统计正确的部分；而mse是统计错误的部分。

## 延伸

![](https://image.jiqizhixin.com/uploads/editor/45bf00ba-ce0d-411e-a1f7-e685d68b98c8/1529558773847.png)

### 平均绝对值误差（也称L1损失）(MAE)

![](https://image.jiqizhixin.com/uploads/editor/bd4624ba-f00c-42aa-b06d-374b74aae671/1529558773392.png)

平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标值和预测值之差的绝对值之和。其只衡量了预测值误差的平均模长，而不考虑方向，取值范围也是从0到正无穷（如果考虑方向，则是残差/误差的总和——平均偏差（MBE））

![](https://image.jiqizhixin.com/uploads/editor/12342b51-bd31-4d68-88ce-f45a616f1192/1529558773472.png)

>### MSE（L2损失）与MAE（L1损失）的比较

简单来说，MSE计算简便，但MAE对异常点有更好的鲁棒性。下面就来介绍导致二者差异的原因。

就是说相对于使用MAE计算损失，使用MSE的模型会赋予异常点更大的权重。所以说如果训练数据被异常点所污染，那么MAE损失就更好用（比如，在训练数据中存在大量错误的反例和正例标记，但是在测试集中没有这个问题）。

但是对于CNN来说，MAE存在一个严重的问题（特别是对于神经网络）：更新的梯度始终相同，也就是说，即使对于很小的损失值，梯度也很大。这样不利于模型的学习。为了解决这个缺陷，我们可以使用变化的学习率，在损失接近最小值时降低学习率。

---

## 小知识

### DropOff:

- 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。
- 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。
- native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。
- 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。

### 梯度消失：

    神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0。造成学习停止。

    比如说：

![](https://static.oschina.net/uploads/img/201709/05200442_by6j.png)

    sigmoid 函数中 最大值是0.25 而且一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布，所以说
    
![](https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25)

    多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新

### 梯度爆炸

    也就是说如果 [公式] 时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大

## 解决方法：

1. Tanh + BatchNormalization
2. Relu

    Relu主要贡献在于：

    - 解决了梯度消失、爆炸的问题
    - 计算方便，计算速度快
    - 加速了网络的训练

    同时也存在一些缺点：

    - 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
    - 输出不是以0为中心的

3. 当然啦 RNN 里面 LSTM/GRU 门的设计能解决问题

---

### Deep Learning 层数的影响：

网络够深(Neurons 足够多)的时候，可以避开较差Local Optima
因为网络层数越多，它能探索到的范围就越大，它的极值点就越靠近全局最优点

### Activation Function

- ReLU. 简单高效，小于0无导数。
- Sigmoid，Tanh函数，logistic regression里经常用，容易计算。但是只在接近0的时候导数比较大

![](https://pic4.zhimg.com/80/v2-665f3304d409b17471dd0b7258818e0a_720w.jpg)

### 过拟合

1. 引入正则化（L1，L2）
2. Dropout
3. 提前终止训练
4. 增加样本量

### BN

我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。
对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。
我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch Normalization.

在每次mini-batch反向传播之后重新对参数进行0均值1方差标准化。这样可以使用更大的学习速率，以及花费更少的精力在参数初始化点上。Batch normalization充当着正则化、减少甚至消除掉Dropout的必要性。

Paper: Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift

### Noise

[Noise](https://flashgene.com/archives/48361.html)